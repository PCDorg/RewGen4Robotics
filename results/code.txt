@torch.jit.script
def compute_reward(
    robot_pos: torch.Tensor, 
    target_pos: torch.Tensor,
    robot_vel: torch.Tensor
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    # Calculate the distance to the target
    distance_to_target = torch.norm(robot_pos - target_pos, dim=-1)

    # Reward for moving closer to the target
    distance_reward = -distance_to_target

    # Encourage forward velocity towards the target
    velocity_to_target = torch.norm(robot_vel, dim=-1)
    velocity_reward = velocity_to_target

    # Stability penalty: minimize velocity deviations (to encourage stable walking)
    stability_penalty = -torch.std(robot_vel, dim=-1)

    # Temperature parameters for transforming reward components
    distance_temperature = 1.0
    velocity_temperature = 1.0
    stability_temperature = 0.1

    # Transform reward components
    distance_reward_transformed = torch.exp(distance_temperature * distance_reward)
    velocity_reward_transformed = torch.exp(velocity_temperature * velocity_reward)
    stability_penalty_transformed = torch.exp(stability_temperature * stability_penalty)

    # Total reward
    total_reward = distance_reward_transformed + velocity_reward_transformed + stability_penalty_transformed

    # Create dictionary of individual reward components
    reward_components = {
        'distance_reward': distance_reward_transformed,
        'velocity_reward': velocity_reward_transformed,
        'stability_penalty': stability_penalty_transformed
    }

    return total_reward, reward_components