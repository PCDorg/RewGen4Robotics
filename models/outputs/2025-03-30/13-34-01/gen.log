[2025-03-30 13:34:01,490][root][INFO] - Using results directory: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30
[2025-03-30 13:34:01,492][root][INFO] - ========== Starting Iteration 1/5 ==========
[2025-03-30 13:34:01,492][root][INFO] - Iteration 1/5: LLM Attempt 1/10
[2025-03-30 13:34:01,492][root][INFO] - Generating new reward function with LLM...
[2025-03-30 13:34:01,493][root][INFO] - Sending request to OpenAI API...
[2025-03-30 13:34:16,021][httpx][INFO] - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-03-30 13:34:16,073][root][INFO] - Received response from LLM.
[2025-03-30 13:34:16,073][root][INFO] - Iteration 1/5: Successfully generated and extracted reward code.
[2025-03-30 13:34:16,073][root][INFO] - Iteration 1/5: Saving generated reward function to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_0.py
[2025-03-30 13:34:16,074][root][INFO] - Iteration 1/5: Starting training and evaluation...
[2025-03-30 13:34:16,074][root][INFO] - Iteration 1: Creating environment with reward from: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_0.py
[2025-03-30 13:34:16,511][root][INFO] - Iteration 1: Initializing PPO model. Logging TensorBoard to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/tensorboard_logs/iteration_0
[2025-03-30 13:34:20,022][root][INFO] - Iteration 1: Starting training for 100000 timesteps...
[2025-03-30 13:34:22,068][root][ERROR] - Error during training/evaluation in Iteration 1: too many values to unpack (expected 2)
[2025-03-30 13:34:22,076][root][ERROR] - Traceback (most recent call last):
  File "/home/ken2/PCD/gen.py", line 139, in train_and_evaluate
    model.learn(total_timesteps=total_timesteps, tb_log_name="PPO", reset_num_timesteps=False)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 207, in step
    return self.step_wait()
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/ken2/PCD/envs/fetchReach.py", line 23, in step
    custom_reward = self.custom_reward_function(next_obs, action, terminated, self.env)
  File "/home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_0.py", line 12, in custom_reward_function
    goal_x, goal_y = env.unwrapped.goal
ValueError: too many values to unpack (expected 2)

[2025-03-30 13:34:22,077][root][WARNING] - Iteration 1/5: Training/Evaluation failed. Error logged.
[2025-03-30 13:34:22,077][root][INFO] - Iteration 1/5: Logging results to /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/conversation_history.md
[2025-03-30 13:34:22,077][root][INFO] - ========== Finished Iteration 1/5 ==========

[2025-03-30 13:34:22,078][root][INFO] - ========== Starting Iteration 2/5 ==========
[2025-03-30 13:34:22,078][root][INFO] - Iteration 2/5: LLM Attempt 1/10
[2025-03-30 13:34:22,078][root][INFO] - Generating new reward function with LLM...
[2025-03-30 13:34:22,082][root][INFO] - Sending request to OpenAI API...
[2025-03-30 13:34:43,553][httpx][INFO] - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-03-30 13:34:43,592][root][INFO] - Received response from LLM.
[2025-03-30 13:34:43,592][root][INFO] - Iteration 2/5: Successfully generated and extracted reward code.
[2025-03-30 13:34:43,592][root][INFO] - Iteration 2/5: Saving generated reward function to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_1.py
[2025-03-30 13:34:43,593][root][INFO] - Iteration 2/5: Starting training and evaluation...
[2025-03-30 13:34:43,593][root][INFO] - Iteration 2: Creating environment with reward from: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_1.py
[2025-03-30 13:34:43,854][root][INFO] - Iteration 2: Initializing PPO model. Logging TensorBoard to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/tensorboard_logs/iteration_1
[2025-03-30 13:34:43,939][root][INFO] - Iteration 2: Starting training for 100000 timesteps...
[2025-03-30 13:34:45,525][root][ERROR] - Error during training/evaluation in Iteration 2: too many values to unpack (expected 2)
[2025-03-30 13:34:45,534][root][ERROR] - Traceback (most recent call last):
  File "/home/ken2/PCD/gen.py", line 139, in train_and_evaluate
    model.learn(total_timesteps=total_timesteps, tb_log_name="PPO", reset_num_timesteps=False)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 207, in step
    return self.step_wait()
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/ken2/PCD/envs/fetchReach.py", line 23, in step
    custom_reward = self.custom_reward_function(next_obs, action, terminated, self.env)
  File "/home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_1.py", line 14, in custom_reward_function
    goal_x, goal_y = env.unwrapped.goal
ValueError: too many values to unpack (expected 2)

[2025-03-30 13:34:45,536][root][WARNING] - Iteration 2/5: Training/Evaluation failed. Error logged.
[2025-03-30 13:34:45,536][root][INFO] - Iteration 2/5: Logging results to /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/conversation_history.md
[2025-03-30 13:34:45,536][root][INFO] - ========== Finished Iteration 2/5 ==========

[2025-03-30 13:34:45,536][root][INFO] - ========== Starting Iteration 3/5 ==========
[2025-03-30 13:34:45,537][root][INFO] - Iteration 3/5: LLM Attempt 1/10
[2025-03-30 13:34:45,537][root][INFO] - Generating new reward function with LLM...
[2025-03-30 13:34:45,542][root][INFO] - Sending request to OpenAI API...
[2025-03-30 13:34:58,770][httpx][INFO] - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-03-30 13:34:58,801][root][INFO] - Received response from LLM.
[2025-03-30 13:34:58,802][root][INFO] - Iteration 3/5: Successfully generated and extracted reward code.
[2025-03-30 13:34:58,802][root][INFO] - Iteration 3/5: Saving generated reward function to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_2.py
[2025-03-30 13:34:58,804][root][INFO] - Iteration 3/5: Starting training and evaluation...
[2025-03-30 13:34:58,804][root][INFO] - Iteration 3: Creating environment with reward from: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_2.py
[2025-03-30 13:34:59,138][root][INFO] - Iteration 3: Initializing PPO model. Logging TensorBoard to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/tensorboard_logs/iteration_2
[2025-03-30 13:34:59,344][root][INFO] - Iteration 3: Starting training for 100000 timesteps...
[2025-03-30 13:35:01,487][root][ERROR] - Error during training/evaluation in Iteration 3: operands could not be broadcast together with shapes (3,) (2,) 
[2025-03-30 13:35:01,490][root][ERROR] - Traceback (most recent call last):
  File "/home/ken2/PCD/gen.py", line 139, in train_and_evaluate
    model.learn(total_timesteps=total_timesteps, tb_log_name="PPO", reset_num_timesteps=False)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 207, in step
    return self.step_wait()
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/ken2/PCD/envs/fetchReach.py", line 23, in step
    custom_reward = self.custom_reward_function(next_obs, action, terminated, self.env)
  File "/home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_2.py", line 18, in custom_reward_function
    dist_to_goal = np.linalg.norm(goal_position - ant_position) # Euclidean distance
ValueError: operands could not be broadcast together with shapes (3,) (2,) 

[2025-03-30 13:35:01,490][root][WARNING] - Iteration 3/5: Training/Evaluation failed. Error logged.
[2025-03-30 13:35:01,490][root][INFO] - Iteration 3/5: Logging results to /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/conversation_history.md
[2025-03-30 13:35:01,491][root][INFO] - ========== Finished Iteration 3/5 ==========

[2025-03-30 13:35:01,491][root][INFO] - ========== Starting Iteration 4/5 ==========
[2025-03-30 13:35:01,491][root][INFO] - Iteration 4/5: LLM Attempt 1/10
[2025-03-30 13:35:01,491][root][INFO] - Generating new reward function with LLM...
[2025-03-30 13:35:01,495][root][INFO] - Sending request to OpenAI API...
[2025-03-30 13:35:14,660][httpx][INFO] - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-03-30 13:35:14,914][root][INFO] - Received response from LLM.
[2025-03-30 13:35:14,914][root][INFO] - Iteration 4/5: Successfully generated and extracted reward code.
[2025-03-30 13:35:14,915][root][INFO] - Iteration 4/5: Saving generated reward function to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_3.py
[2025-03-30 13:35:14,915][root][INFO] - Iteration 4/5: Starting training and evaluation...
[2025-03-30 13:35:14,916][root][INFO] - Iteration 4: Creating environment with reward from: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_3.py
[2025-03-30 13:35:15,171][root][INFO] - Iteration 4: Initializing PPO model. Logging TensorBoard to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/tensorboard_logs/iteration_3
[2025-03-30 13:35:15,212][root][INFO] - Iteration 4: Starting training for 100000 timesteps...
[2025-03-30 13:35:15,653][root][ERROR] - Error during training/evaluation in Iteration 4: operands could not be broadcast together with shapes (3,) (2,) 
[2025-03-30 13:35:15,653][root][ERROR] - Traceback (most recent call last):
  File "/home/ken2/PCD/gen.py", line 139, in train_and_evaluate
    model.learn(total_timesteps=total_timesteps, tb_log_name="PPO", reset_num_timesteps=False)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 207, in step
    return self.step_wait()
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/ken2/PCD/envs/fetchReach.py", line 23, in step
    custom_reward = self.custom_reward_function(next_obs, action, terminated, self.env)
  File "/home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_3.py", line 16, in custom_reward_function
    distance_to_goal = np.linalg.norm(target_position - ant_position)
ValueError: operands could not be broadcast together with shapes (3,) (2,) 

[2025-03-30 13:35:15,654][root][WARNING] - Iteration 4/5: Training/Evaluation failed. Error logged.
[2025-03-30 13:35:15,654][root][INFO] - Iteration 4/5: Logging results to /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/conversation_history.md
[2025-03-30 13:35:15,656][root][INFO] - ========== Finished Iteration 4/5 ==========

[2025-03-30 13:35:15,656][root][INFO] - ========== Starting Iteration 5/5 ==========
[2025-03-30 13:35:15,656][root][INFO] - Iteration 5/5: LLM Attempt 1/10
[2025-03-30 13:35:15,657][root][INFO] - Generating new reward function with LLM...
[2025-03-30 13:35:15,659][root][INFO] - Sending request to OpenAI API...
[2025-03-30 13:35:29,995][httpx][INFO] - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
[2025-03-30 13:35:30,000][root][INFO] - Received response from LLM.
[2025-03-30 13:35:30,000][root][INFO] - Iteration 5/5: Successfully generated and extracted reward code.
[2025-03-30 13:35:30,000][root][INFO] - Iteration 5/5: Saving generated reward function to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_4.py
[2025-03-30 13:35:30,001][root][INFO] - Iteration 5/5: Starting training and evaluation...
[2025-03-30 13:35:30,001][root][INFO] - Iteration 5: Creating environment with reward from: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_4.py
[2025-03-30 13:35:30,247][root][INFO] - Iteration 5: Initializing PPO model. Logging TensorBoard to: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/tensorboard_logs/iteration_4
[2025-03-30 13:35:30,251][root][INFO] - Iteration 5: Starting training for 100000 timesteps...
[2025-03-30 13:35:30,850][root][ERROR] - Error during training/evaluation in Iteration 5: operands could not be broadcast together with shapes (2,) (3,) 
[2025-03-30 13:35:30,854][root][ERROR] - Traceback (most recent call last):
  File "/home/ken2/PCD/gen.py", line 139, in train_and_evaluate
    model.learn(total_timesteps=total_timesteps, tb_log_name="PPO", reset_num_timesteps=False)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 207, in step
    return self.step_wait()
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/ken2/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/ken2/PCD/envs/fetchReach.py", line 23, in step
    custom_reward = self.custom_reward_function(next_obs, action, terminated, self.env)
  File "/home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/reward_function_4.py", line 18, in custom_reward_function
    distance_to_goal = np.linalg.norm(position - goal)
ValueError: operands could not be broadcast together with shapes (2,) (3,) 

[2025-03-30 13:35:30,854][root][WARNING] - Iteration 5/5: Training/Evaluation failed. Error logged.
[2025-03-30 13:35:30,855][root][INFO] - Iteration 5/5: Logging results to /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30/conversation_history.md
[2025-03-30 13:35:30,855][root][INFO] - ========== Finished Iteration 5/5 ==========

[2025-03-30 13:35:30,855][root][INFO] - Iterative reward function generation complete.
[2025-03-30 13:35:30,855][root][INFO] - Final Results Summary:
[2025-03-30 13:35:30,855][root][INFO] -   Iter 1: Status='Failed - Error during Training/Evaluation', Eval Reward=N/A, Train Reward=N/A, Model Saved='False', Error='True'
[2025-03-30 13:35:30,855][root][INFO] -   Iter 2: Status='Failed - Error during Training/Evaluation', Eval Reward=N/A, Train Reward=N/A, Model Saved='False', Error='True'
[2025-03-30 13:35:30,855][root][INFO] -   Iter 3: Status='Failed - Error during Training/Evaluation', Eval Reward=N/A, Train Reward=N/A, Model Saved='False', Error='True'
[2025-03-30 13:35:30,856][root][INFO] -   Iter 4: Status='Failed - Error during Training/Evaluation', Eval Reward=N/A, Train Reward=N/A, Model Saved='False', Error='True'
[2025-03-30 13:35:30,856][root][INFO] -   Iter 5: Status='Failed - Error during Training/Evaluation', Eval Reward=N/A, Train Reward=N/A, Model Saved='False', Error='True'
[2025-03-30 13:35:30,856][root][INFO] - Detailed logs and artifacts saved in: /home/ken2/PCD/outputs/2025-03-30/13-34-01/results/2025-03-30
