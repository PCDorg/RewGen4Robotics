# @package _global_

defaults:
  - env: fetchReach # Default environment
  - override /hydra/job_logging: disabled # Optional: disable Hydra's default logging structure if desired
  - override /hydra/hydra_logging: disabled # Optional: disable Hydra's default logging structure if desired

# General experiment settings
iterations: ${experiment.iterations} # Use iterations from experiment section
total_timesteps: ${env.timestamps} # Use timestamp from the selected env config
eval_episodes: ${training.eval_episodes} # Use eval_episodes from training section
# target_reward: 100.0 # Example target reward, may need adjustment per env (Removed, consider per-env target)
final_training: ${experiment.final_training} # Use final_training from experiment section
# selected_reward_function: "reward_function_4.py" # Example, which reward function to use for final training (Removed, usually determined by last iteration)

# LLM settings (Moved to llm section below)
# model: "gpt-4" # Or your preferred model

# Paths (Consider making these relative or using Hydra variables if possible)
# Note: Absolute paths might cause issues if the project is moved.
# Using relative paths from the project root or Hydra's output directory is safer.
# Example using Hydra's interpolation for output dirs:
# results_base: ${hydra:runtime.output_dir} # Saves results relative to Hydra's output folder
# prompt_base: utils/prompts # Relative path

# --- Training settings (if needed globally, otherwise put in env configs) ---
# Example: algorithm specific params (can be overridden by env config)
# algorithm:
#   name: SAC
#   learning_rate: 3e-4
#   buffer_size: 1000000
#   batch_size: 256
#   gamma: 0.99
#   tau: 0.005

# The 'env' settings will be loaded from the selected file in cfg/env/

# Training parameters
training:
  algorithm: "SAC"  # RL algorithm to use
  total_timesteps: 3000000  # Training timesteps for each iteration
  eval_episodes: 5  # Number of evaluation episodes
  learning_rate: 0.0003
  buffer_size: 1000000
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  ent_coef: auto
  sac_params:
    learning_rate: 0.0003
    buffer_size: 1000000
    learning_starts: 10000
    batch_size: 256
    tau: 0.005
    gamma: 0.99

# Environment parameters (This section seems empty or misplaced, remove if not used)
# llm_provider: groq          # Can be 'openai' or 'groq' (Moved to llm section)
# model: llama3-8b-8192     # (Moved to llm section)

# LLM parameters
llm:
  provider: groq          # Can be 'openai' or 'groq'
  model: llama3-8b-8192   # Model name appropriate for the provider
  temperature: 1
  max_tokens: 2000
  max_retries: 3

# Experiment parameters
experiment:
  iterations: 5  # Number of iterative cycles for reward generation
  final_training: false  # Whether to do final training after reward generation
  save_frequency: 10000  # Save model every n timesteps
  tensorboard_log: true  # Whether to log to tensorboard
  verbose: 1  # Verbosity level

# Paths
paths:
  results_dir: "outputs"  # Base directory for all run results
  model_dir: "models"    # Base directory for all saved models
# tensorboard_dir: "tensorboard_logs" # Removed, will be relative to run results
  prompt_dir: "utils/prompts" # Relative path to prompt files

env:
  env_name: "Go1MujocoCustom-v0"
  task: "Walk forward"
  task_description: "Train Go1 to walk forward tracking velocity."
  ctrl_type: "torque"

evaluation:
  eval_episodes: 10 # Number of episodes for final evaluation

# --- Path to the specific reward function file for offline training ---
# REWARD FUNCTION PATH MUST BE PROVIDED VIA COMMAND LINE FOR train_offline.py
# Example: python train_offline.py reward_function_path="path/to/your/reward_function.py"
# reward_function_path: "/home/ken2/PCD/outputs/2025-05-05/18-00-18/results/Go1MujocoCustom-v0/2025-05-05_18-00-18/reward_function_0.py"

# Optional: Add render flag if you want to watch during training
render: True